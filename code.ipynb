{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPK0ETwpVqJ5gWslW2CI0H7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hari-Priya-18/B6_PFDS_1372/blob/main/code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# --- Step 1: Data Acquisition and Merging ---\n",
        "# This framework assumes multiple survey-based datasets are available in CSV format.\n",
        "# A key challenge is the heterogeneity of data sources.[1, 2]\n",
        "# This step involves loading and carefully merging them based on common features.\n",
        "\n",
        "def load_and_merge_datasets(file_paths):\n",
        "    \"\"\"Loads multiple datasets and merges them into a single DataFrame.\"\"\"\n",
        "    all_data = [] # Corrected indentation and initialization\n",
        "    for path in file_paths:\n",
        "        df = pd.read_csv(path)\n",
        "        # Standardize column names (e.g., 'academic_pressure', 'sleep_quality')\n",
        "        # to ensure consistency across datasets.\n",
        "        # This is a crucial step to address data fragmentation.[2]\n",
        "        all_data.append(df)\n",
        "\n",
        "    # Concatenate all dataframes.\n",
        "    merged_df = pd.concat(all_data, ignore_index=True)\n",
        "    return merged_df\n",
        "\n",
        "# --- Step 2: Data Preprocessing and Feature Engineering ---\n",
        "# The abstract mentions survey-based academic, lifestyle, behavioral, and health features.\n",
        "# Research confirms the importance of a holistic view of student life.[3]\n",
        "# This step involves cleaning the data and preparing it for the models.\n",
        "\n",
        "def preprocess_data(df):\n",
        "    \"\"\"Handles data preprocessing including cleaning, imputation, and feature creation.\"\"\"\n",
        "    # Define features based on research insights.[3, 4]\n",
        "    # Updated column names based on the available columns in the merged DataFrame\n",
        "    academic_features = ['Academic Pressure', 'Work Pressure', 'CGPA']\n",
        "    lifestyle_features = ['Sleep Duration', 'Dietary Habits', 'Social Media Usage (Hours per day)'] # Using Social Media Usage as a proxy for social time\n",
        "    health_features = ['anxiety_level', 'self_esteem', 'mental_health_history']\n",
        "\n",
        "    # Target variables for joint prediction.\n",
        "    # Updated column names based on the available columns in the merged DataFrame\n",
        "    target_variables = ['stress_level', 'Depression']\n",
        "\n",
        "    # Separate features and targets.\n",
        "    # Select only the columns that exist in the DataFrame\n",
        "    all_features = academic_features + lifestyle_features + health_features\n",
        "    existing_features = [col for col in all_features if col in df.columns]\n",
        "\n",
        "    if not existing_features:\n",
        "        print(\"Error: No specified feature columns were found in the DataFrame.\")\n",
        "        print(\"Please check the column names in your datasets and update the feature lists in preprocess_data.\")\n",
        "        return None, None, None, None, None # Return None values to indicate failure\n",
        "\n",
        "    X = df[existing_features]\n",
        "\n",
        "    existing_targets = [col for col in target_variables if col in df.columns]\n",
        "    y = df[existing_targets]\n",
        "\n",
        "    if not existing_targets:\n",
        "        print(\"Error: No specified target columns were found in the DataFrame.\")\n",
        "        print(\"Please check the column names in your datasets and update the target lists in preprocess_data.\")\n",
        "        return None, None, None, None, None # Return None values to indicate failure\n",
        "\n",
        "\n",
        "    # Split data into training and testing sets (e.g., 80:20 ratio).[5]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Identify numerical and categorical features.\n",
        "    numerical_features = X.select_dtypes(include=np.number).columns\n",
        "    categorical_features = X.select_dtypes(include='object').columns\n",
        "\n",
        "    # Create preprocessing pipelines for numerical and categorical data.\n",
        "    # Imputation handles missing values.[5]\n",
        "    numerical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='mean')), # Corrected initialization and indentation\n",
        "                                            ('scaler', StandardScaler())]) # Corrected indentation\n",
        "\n",
        "    categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')), # Corrected initialization and indentation\n",
        "                                              ('onehot', OneHotEncoder(handle_unknown='ignore'))]) # Corrected indentation\n",
        "\n",
        "    # Combine transformers using ColumnTransformer.\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numerical_transformer, numerical_features),\n",
        "            ('cat', categorical_transformer, categorical_features)\n",
        "        ])\n",
        "\n",
        "    return preprocessor, X_train, X_test, y_train, y_test\n",
        "\n",
        "# --- Corrected Step 3: Handling Class Imbalance (e.g., with SMOTE) ---\n",
        "# Mental health datasets are often imbalanced. SMOTE is a common technique to mitigate this issue.[1, 6]\n",
        "\n",
        "def apply_smote(X_train, y_train):\n",
        "    \"\"\"Applies SMOTE to the training data to address class imbalance.\"\"\"\n",
        "    smote = SMOTE(random_state=42)\n",
        "    # The target variable must be a 2D array. We reshape it here.\n",
        "    # NOTE: Assuming 'Depression' is a target variable and is suitable for SMOTE.\n",
        "    # You may need to adjust this based on which target variable you want to resample.\n",
        "    if 'Depression' not in y_train.columns:\n",
        "         print(\"Error: 'Depression' column not found in the target variables for SMOTE.\")\n",
        "         return X_train, y_train # Return original data if target not found\n",
        "\n",
        "    X_resampled, y_resampled = smote.fit_resample(X_train, y_train['Depression'])\n",
        "    return X_resampled, y_resampled\n",
        "\n",
        "# --- Corrected Step 4: Model Training and Evaluation ---\n",
        "# The abstract lists five models: Logistic Regression, Random Forest, SVM, XGBoost, and an ANN.\n",
        "# We will evaluate each using the specified metrics.[7]\n",
        "\n",
        "def train_and_evaluate_models(preprocessor, X_train, X_test, y_train, y_test):\n",
        "    \"\"\"Trains and evaluates the proposed machine learning models.\"\"\"\n",
        "    # Check if preprocessing was successful and data is available\n",
        "    if X_train is None or y_train is None or X_test is None or y_test is None or preprocessor is None:\n",
        "        print(\"Model training skipped due to missing data or preprocessor.\")\n",
        "        return {}\n",
        "\n",
        "    models = {\n",
        "        'Logistic Regression': LogisticRegression(max_iter=1000), # Good baseline model.[8]\n",
        "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42), # Robust and high-performing.[9, 6]\n",
        "        'SVM': SVC(probability=True, random_state=42), # Effective in high-dimensional spaces.[10]\n",
        "        'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42), # Superior performance is consistently reported.[6]\n",
        "        'ANN': Sequential([ # Corrected indentation\n",
        "            Dense(64, activation='relu'), # Corrected indentation\n",
        "            Dense(32, activation='relu'), # Corrected indentation\n",
        "            Dense(1, activation='sigmoid') # Sigmoid for binary classification (stress/depression). # Corrected indentation\n",
        "        ])\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # NOTE: The original code assumed a single target ('depression') for training and evaluation.\n",
        "    # Since the target variables are now 'stress_level' and 'Depression', this needs to be handled.\n",
        "    # For simplicity and to avoid a multi-label classification setup which requires significant changes,\n",
        "    # I will train and evaluate models for 'Depression' as the target variable, similar to the original code's structure.\n",
        "    # If you need to predict both 'stress_level' and 'Depression' jointly, a multi-label approach is required.\n",
        "\n",
        "    target_column_for_training = 'Depression' # Change to 'stress_level' if needed\n",
        "\n",
        "    if target_column_for_training not in y_train.columns or target_column_for_training not in y_test.columns:\n",
        "        print(f\"Error: Target column '{target_column_for_training}' not found in the target variables in either train or test sets.\")\n",
        "        return {}\n",
        "\n",
        "    # Drop rows with NaN in the target variable from training data\n",
        "    train_data = pd.concat([X_train, y_train], axis=1)\n",
        "    train_data.dropna(subset=[target_column_for_training], inplace=True)\n",
        "    X_train_cleaned = train_data[X_train.columns]\n",
        "    y_train_cleaned = train_data[target_column_for_training]\n",
        "\n",
        "\n",
        "    for name, model in models.items():\n",
        "        print(f\"Training {name}...\")\n",
        "\n",
        "        # Create a pipeline that first preprocesses, then trains the model.\n",
        "        # This ensures consistency in the data transformation process.\n",
        "        if name == 'ANN':\n",
        "            # ANN requires a different training approach.\n",
        "            X_train_processed = preprocessor.fit_transform(X_train_cleaned)\n",
        "            X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "            # Dynamically set the input shape for the ANN based on the preprocessed data\n",
        "            models[name] = Sequential([\n",
        "                Dense(64, activation='relu', input_shape=(X_train_processed.shape[1],)),\n",
        "                Dense(32, activation='relu'),\n",
        "                Dense(1, activation='sigmoid')\n",
        "            ])\n",
        "            model = models[name] # Update the model variable to the new Sequential model\n",
        "\n",
        "            model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "            # Train on the selected target column\n",
        "            model.fit(X_train_processed, y_train_cleaned, epochs=10, batch_size=32, verbose=0)\n",
        "            y_pred_proba = model.predict(X_test_processed).flatten()\n",
        "            y_pred = (y_pred_proba > 0.5).astype(int)\n",
        "        else:\n",
        "            pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                                       ('classifier', model)])\n",
        "\n",
        "            # Train on the selected target column\n",
        "            pipeline.fit(X_train_cleaned, y_train_cleaned)\n",
        "            y_pred = pipeline.predict(X_test)\n",
        "            y_pred_proba = pipeline.predict_proba(X_test)[:, 1] if hasattr(pipeline.named_steps['classifier'], 'predict_proba') else None\n",
        "\n",
        "        # --- Model Evaluation ---\n",
        "        # Evaluate using the metrics from the abstract: accuracy, precision, recall, F1-score, and ROC-AUC.[7]\n",
        "        # These metrics provide a \"balanced view of model reliability\".[7]\n",
        "\n",
        "        # Align y_test and y_pred by dropping rows with NaN in y_test\n",
        "        y_test_cleaned = y_test[target_column_for_training].dropna()\n",
        "        y_pred_cleaned = pd.Series(y_pred, index=y_test.index).loc[y_test_cleaned.index]\n",
        "        y_pred_proba_cleaned = pd.Series(y_pred_proba, index=y_test.index).loc[y_test_cleaned.index] if y_pred_proba is not None else None\n",
        "\n",
        "\n",
        "        # Evaluate against the selected target column\n",
        "        accuracy = accuracy_score(y_test_cleaned, y_pred_cleaned)\n",
        "        precision = precision_score(y_test_cleaned, y_pred_cleaned, zero_division=0)\n",
        "        recall = recall_score(y_test_cleaned, y_pred_cleaned, zero_division=0)\n",
        "        f1 = f1_score(y_test_cleaned, y_pred_cleaned, zero_division=0)\n",
        "        roc_auc = roc_auc_score(y_test_cleaned, y_pred_proba_cleaned) if y_pred_proba_cleaned is not None else 'N/A'\n",
        "\n",
        "\n",
        "        results[name] = {\n",
        "            'Accuracy': accuracy,\n",
        "            'Precision': precision,\n",
        "            'Recall': recall,\n",
        "            'F1-score': f1,\n",
        "            'ROC-AUC': roc_auc\n",
        "        }\n",
        "\n",
        "    return results\n",
        "\n",
        "# --- Main Execution Block ---\n",
        "if __name__ == '__main__':\n",
        "    # You must update this list with your actual file paths and ensure the columns match the code.\n",
        "    file_paths = ['/content/student_depression_dataset.csv', '/content/StressLevelDataset.csv', '/content/Student_Mental_Stress_and_Coping_Mechanisms.csv', '/content/Mental Health Dataset.csv']\n",
        "\n",
        "    # Load and preprocess the data.\n",
        "    df = load_and_merge_datasets(file_paths)\n",
        "    preprocessor, X_train, X_test, y_train, y_test = preprocess_data(df)\n",
        "\n",
        "    # Train and evaluate the models.\n",
        "    results = train_and_evaluate_models(preprocessor, X_train, X_test, y_train, y_test)\n",
        "\n",
        "    # Print the performance results.\n",
        "    print(\"Model Performance Results:\")\n",
        "    for model, metrics in results.items():\n",
        "        print(f\"\\n--- {model} ---\")\n",
        "        for metric, value in metrics.items():\n",
        "            # Check if the value is a number before formatting\n",
        "            if isinstance(value, (int, float)):\n",
        "                print(f\"{metric}: {value:.4f}\")\n",
        "            else:\n",
        "                print(f\"{metric}: value\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svsniH9YSyBn",
        "outputId": "5a927b05-a98a-4067-9066-adbe4a345c5f"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Logistic Regression...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['Social Media Usage (Hours per day)' 'anxiety_level' 'self_esteem'\n",
            " 'mental_health_history']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['Social Media Usage (Hours per day)' 'anxiety_level' 'self_esteem'\n",
            " 'mental_health_history']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['Social Media Usage (Hours per day)' 'anxiety_level' 'self_esteem'\n",
            " 'mental_health_history']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Random Forest...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['Social Media Usage (Hours per day)' 'anxiety_level' 'self_esteem'\n",
            " 'mental_health_history']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['Social Media Usage (Hours per day)' 'anxiety_level' 'self_esteem'\n",
            " 'mental_health_history']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['Social Media Usage (Hours per day)' 'anxiety_level' 'self_esteem'\n",
            " 'mental_health_history']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training SVM...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['Social Media Usage (Hours per day)' 'anxiety_level' 'self_esteem'\n",
            " 'mental_health_history']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['Social Media Usage (Hours per day)' 'anxiety_level' 'self_esteem'\n",
            " 'mental_health_history']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['Social Media Usage (Hours per day)' 'anxiety_level' 'self_esteem'\n",
            " 'mental_health_history']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training XGBoost...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['Social Media Usage (Hours per day)' 'anxiety_level' 'self_esteem'\n",
            " 'mental_health_history']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/xgboost/training.py:183: UserWarning: [06:12:33] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['Social Media Usage (Hours per day)' 'anxiety_level' 'self_esteem'\n",
            " 'mental_health_history']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['Social Media Usage (Hours per day)' 'anxiety_level' 'self_esteem'\n",
            " 'mental_health_history']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training ANN...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['Social Media Usage (Hours per day)' 'anxiety_level' 'self_esteem'\n",
            " 'mental_health_history']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py:635: UserWarning: Skipping features without any observed values: ['Social Media Usage (Hours per day)' 'anxiety_level' 'self_esteem'\n",
            " 'mental_health_history']. At least one non-missing value is needed for imputation with strategy='mean'.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2014/2014\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
            "Model Performance Results:\n",
            "\n",
            "--- Logistic Regression ---\n",
            "Accuracy: 0.7394\n",
            "Precision: 0.7572\n",
            "Recall: 0.8043\n",
            "F1-score: 0.7800\n",
            "ROC-AUC: 0.8030\n",
            "\n",
            "--- Random Forest ---\n",
            "Accuracy: 0.6803\n",
            "Precision: 0.7144\n",
            "Recall: 0.7386\n",
            "F1-score: 0.7263\n",
            "ROC-AUC: 0.7271\n",
            "\n",
            "--- SVM ---\n",
            "Accuracy: 0.7344\n",
            "Precision: 0.7209\n",
            "Recall: 0.8774\n",
            "F1-score: 0.7915\n",
            "ROC-AUC: 0.7865\n",
            "\n",
            "--- XGBoost ---\n",
            "Accuracy: 0.7312\n",
            "Precision: 0.7510\n",
            "Recall: 0.7959\n",
            "F1-score: 0.7728\n",
            "ROC-AUC: 0.7890\n",
            "\n",
            "--- ANN ---\n",
            "Accuracy: 0.7392\n",
            "Precision: 0.7448\n",
            "Recall: 0.8307\n",
            "F1-score: 0.7854\n",
            "ROC-AUC: 0.8027\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOM2NRyOENPyy1dH5/DM8+0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Hari-Priya-18/B6_PFDS_1372/blob/main/Project_code.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "R_8J_6tfO9mi"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.impute import SimpleImputer\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from xgboost import XGBClassifier\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "# --- Step 1: Data Acquisition and Merging ---\n",
        "# This framework assumes multiple survey-based datasets are available in CSV format.\n",
        "# A key challenge is the heterogeneity of data sources.[3, 4]\n",
        "# This step involves loading and carefully merging them based on common features.\n",
        "\n",
        "def load_and_merge_datasets(file_paths):\n",
        "    \"\"\"Loads multiple datasets and merges them into a single DataFrame.\"\"\"\n",
        "    all_data = []\n",
        "    for path in file_paths:\n",
        "        df = pd.read_csv(path)\n",
        "        # Standardize column names (e.g., 'academic_pressure', 'sleep_quality')\n",
        "        # to ensure consistency across datasets.\n",
        "        # This is a crucial step to address data fragmentation.[4]\n",
        "        all_data.append(df)\n",
        "\n",
        "    # Concatenate all dataframes.\n",
        "    merged_df = pd.concat(all_data, ignore_index=True)\n",
        "    return merged_df\n",
        "\n",
        "# Example usage:\n",
        "# file_paths = ['dataset_1.csv', 'dataset_2.csv', 'dataset_3.csv']\n",
        "# df = load_and_merge_datasets(file_paths)\n",
        "\n",
        "# --- Step 2: Data Preprocessing and Feature Engineering ---\n",
        "# The abstract mentions survey-based academic, lifestyle, behavioral, and health features.\n",
        "# Research confirms the importance of a holistic view of student life.[5]\n",
        "# This step involves cleaning the data and preparing it for the models.\n",
        "\n",
        "def preprocess_data(df):\n",
        "    \"\"\"Handles data preprocessing including cleaning, imputation, and feature creation.\"\"\"\n",
        "    # Define features based on research insights.[5, 6]\n",
        "    # NOTE: These column names are based on the available columns in the merged DataFrame.\n",
        "    # You may need to adjust these based on the specific columns in your datasets.\n",
        "    academic_features = ['academic_pressure_placeholder', 'workload_placeholder', 'cgpa_placeholder'] # Replace with actual column names\n",
        "    lifestyle_features = ['sleep_patterns_placeholder', 'eating_habits_placeholder', 'social_time_placeholder'] # Replace with actual column names\n",
        "    health_features = ['Anxiety_Level', 'self_esteem_placeholder', 'mental_health_history_placeholder'] # Replace with actual column names for self_esteem and history\n",
        "\n",
        "    # Target variables for joint prediction.\n",
        "    # Note: These are example column names; they should match the datasets.\n",
        "    # Based on the available columns, 'Depression' and 'stress_level' seem relevant.\n",
        "    target_variables = ['stress_level', 'Depression']\n",
        "\n",
        "    # Separate features and targets.\n",
        "    # Select only the columns that exist in the DataFrame\n",
        "    all_features = academic_features + lifestyle_features + health_features\n",
        "    existing_features = [col for col in all_features if col in df.columns]\n",
        "\n",
        "    if not existing_features:\n",
        "        print(\"Error: No specified feature columns were found in the DataFrame.\")\n",
        "        print(\"Please check the column names in your datasets and update the feature lists in preprocess_data.\")\n",
        "        return None, None, None, None, None # Return None values to indicate failure\n",
        "\n",
        "    X = df[existing_features]\n",
        "\n",
        "    existing_targets = [col for col in target_variables if col in df.columns]\n",
        "    y = df[existing_targets]\n",
        "\n",
        "    if not existing_targets:\n",
        "        print(\"Error: No specified target columns were found in the DataFrame.\")\n",
        "        print(\"Please check the column names in your datasets and update the target lists in preprocess_data.\")\n",
        "        return None, None, None, None, None # Return None values to indicate failure\n",
        "\n",
        "\n",
        "    # Split data into training and testing sets (e.g., 80:20 ratio).[7]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Identify numerical and categorical features.\n",
        "    numerical_features = X.select_dtypes(include=np.number).columns\n",
        "    categorical_features = X.select_dtypes(include='object').columns\n",
        "\n",
        "    # Create preprocessing pipelines for numerical and categorical data.\n",
        "    # Imputation handles missing values.[7]\n",
        "    numerical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='mean')),\n",
        "                                            ('scaler', StandardScaler())])\n",
        "\n",
        "    categorical_transformer = Pipeline(steps=[('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "                                              ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n",
        "\n",
        "    # Combine transformers using ColumnTransformer.\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numerical_transformer, numerical_features),\n",
        "            ('cat', categorical_transformer, categorical_features)\n",
        "        ])\n",
        "\n",
        "    return preprocessor, X_train, X_test, y_train, y_test\n",
        "\n",
        "# --- Step 3: Handling Class Imbalance (e.g., with SMOTE) ---\n",
        "# Mental health datasets are often imbalanced.[3]\n",
        "# SMOTE is a common technique to mitigate this issue.[3, 8]\n",
        "\n",
        "def apply_smote(X_train, y_train):\n",
        "    \"\"\"Applies SMOTE to the training data to address class imbalance.\"\"\"\n",
        "    smote = SMOTE(random_state=42)\n",
        "    # This example handles a single target; a more complex approach would be needed\n",
        "    # for multi-label prediction.\n",
        "    # NOTE: Assuming 'Depression' is a target variable and is suitable for SMOTE.\n",
        "    # You may need to adjust this based on which target variable you want to resample.\n",
        "    if 'Depression' not in y_train.columns:\n",
        "         print(\"Error: 'Depression' column not found in the target variables for SMOTE.\")\n",
        "         return X_train, y_train # Return original data if target not found\n",
        "\n",
        "    X_resampled, y_resampled = smote.fit_resample(X_train, y_train['Depression'])\n",
        "    return X_resampled, y_resampled\n",
        "\n",
        "# --- Step 4: Model Training and Evaluation ---\n",
        "# The abstract lists five models: Logistic Regression, Random Forest, SVM, XGBoost, and an ANN.\n",
        "# We will evaluate each using the specified metrics.[9]\n",
        "\n",
        "def train_and_evaluate_models(preprocessor, X_train, X_test, y_train, y_test):\n",
        "    \"\"\"Trains and evaluates the proposed machine learning models.\"\"\"\n",
        "    # Check if preprocessing was successful and data is available\n",
        "    if X_train is None or y_train is None or X_test is None or y_test is None or preprocessor is None:\n",
        "        print(\"Model training skipped due to missing data or preprocessor.\")\n",
        "        return {}\n",
        "\n",
        "    models = {\n",
        "        'Logistic Regression': LogisticRegression(max_iter=1000), # Good baseline model.[10]\n",
        "        'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42), # Robust and high-performing.[11, 8]\n",
        "        'SVM': SVC(probability=True, random_state=42), # Effective in high-dimensional spaces.[12]\n",
        "        'XGBoost': XGBClassifier(use_label_encoder=False, eval_metric='logloss', random_state=42), # Superior performance is consistently reported.[8]\n",
        "        'ANN': Sequential([\n",
        "            Dense(64, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "            Dense(32, activation='relu'),\n",
        "            Dense(1, activation='sigmoid') # Sigmoid for binary classification (stress/depression).\n",
        "        ])\n",
        "    }\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    # NOTE: The original code assumed a single target ('depression') for training and evaluation.\n",
        "    # Since the target variables are now 'stress_level' and 'Depression', this needs to be handled.\n",
        "    # For simplicity and to avoid a multi-label classification setup which requires significant changes,\n",
        "    # I will train and evaluate models for 'Depression' as the target variable, similar to the original code's structure.\n",
        "    # If you need to predict both 'stress_level' and 'Depression' jointly, a multi-label approach is required.\n",
        "\n",
        "    target_column_for_training = 'Depression' # Change to 'stress_level' if needed\n",
        "\n",
        "    if target_column_for_training not in y_train.columns:\n",
        "        print(f\"Error: Target column '{target_column_for_training}' not found in the target variables.\")\n",
        "        return {}\n",
        "\n",
        "\n",
        "    for name, model in models.items():\n",
        "        print(f\"Training {name}...\")\n",
        "\n",
        "        # Create a pipeline that first preprocesses, then trains the model.\n",
        "        # This ensures consistency in the data transformation process.\n",
        "        pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                                   ('classifier', model)])\n",
        "\n",
        "        # Fit the model and make predictions.\n",
        "        if name == 'ANN':\n",
        "            # ANN requires a different training approach.\n",
        "            X_train_processed = preprocessor.fit_transform(X_train)\n",
        "            X_test_processed = preprocessor.transform(X_test)\n",
        "\n",
        "            model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "            # Train on the selected target column\n",
        "            model.fit(X_train_processed, y_train[target_column_for_training], epochs=10, batch_size=32, verbose=0)\n",
        "            y_pred_proba = model.predict(X_test_processed).flatten()\n",
        "            y_pred = (y_pred_proba > 0.5).astype(int)\n",
        "        else:\n",
        "            # Train on the selected target column\n",
        "            pipeline.fit(X_train, y_train[target_column_for_training])\n",
        "            y_pred = pipeline.predict(X_test)\n",
        "            y_pred_proba = pipeline.predict_proba(X_test)[:, 1] if hasattr(pipeline.named_steps['classifier'], 'predict_proba') else None\n",
        "\n",
        "        # --- Model Evaluation ---\n",
        "        # Evaluate using the metrics from the abstract: accuracy, precision, recall, F1-score, and ROC-AUC.[9]\n",
        "        # These metrics provide a \"balanced view of model reliability\".[9]\n",
        "\n",
        "        # Evaluate against the selected target column\n",
        "        accuracy = accuracy_score(y_test[target_column_for_training], y_pred)\n",
        "        precision = precision_score(y_test[target_column_for_training], y_pred, zero_division=0)\n",
        "        recall = recall_score(y_test[target_column_for_training], y_pred, zero_division=0)\n",
        "        f1 = f1_score(y_test[target_column_for_training], y_pred, zero_division=0)\n",
        "        roc_auc = roc_auc_score(y_test[target_column_for_training], y_pred_proba) if y_pred_proba is not None else 'N/A'\n",
        "\n",
        "\n",
        "        results[name] = {\n",
        "            'Accuracy': accuracy,\n",
        "            'Precision': precision,\n",
        "            'Recall': recall,\n",
        "            'F1-score': f1,\n",
        "            'ROC-AUC': roc_auc\n",
        "        }\n",
        "\n",
        "    return results\n",
        "\n",
        "# Final step: Print the results.\n",
        "# print(\"Model Performance Results:\")\n",
        "# for model, metrics in results.items():\n",
        "#     print(f\"\\n--- {model} ---\")\n",
        "#     for metric, value in metrics.items():\n",
        "#         print(f\"{metric}: {value:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "23f06783",
        "outputId": "df0a8295-4666-41e2-c057-1e10cde69167"
      },
      "source": [
        "# Example usage of the functions:\n",
        "# First, you need to load and preprocess the data.\n",
        "# Replace with your actual file paths\n",
        "file_paths = ['/content/student_depression_dataset.csv', '/content/StressLevelDataset.csv', '/content/Student_Mental_Stress_and_Coping_Mechanisms.csv', '/content/Mental Health Dataset.csv']\n",
        "df = load_and_merge_datasets(file_paths)\n",
        "\n",
        "# Print column names to help identify correct features and targets\n",
        "print(\"DataFrame columns:\")\n",
        "print(df.columns)\n",
        "\n",
        "# Preprocess the data\n",
        "preprocessor, X_train, X_test, y_train, y_test = preprocess_data(df)\n",
        "\n",
        "# Apply SMOTE (optional, uncomment if needed and adjust for multi-label if necessary)\n",
        "# X_resampled, y_resampled = apply_smote(X_train, y_train)\n",
        "\n",
        "# Train and evaluate the models\n",
        "results = train_and_evaluate_models(preprocessor, X_train, X_test, y_train, y_test)\n",
        "\n",
        "# Print the results\n",
        "print(\"Model Performance Results:\")\n",
        "for model, metrics in results.items():\n",
        "    print(f\"\\n--- {model} ---\")\n",
        "    for metric, value in metrics.items():\n",
        "        print(f\"{metric}: {value:.4f}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DataFrame columns:\n",
            "Index(['id', 'Gender', 'Age', 'City', 'Profession', 'Academic Pressure',\n",
            "       'Work Pressure', 'CGPA', 'Study Satisfaction', 'Job Satisfaction',\n",
            "       'Sleep Duration', 'Dietary Habits', 'Degree',\n",
            "       'Have you ever had suicidal thoughts ?', 'Work/Study Hours',\n",
            "       'Financial Stress', 'Family History of Mental Illness', 'Depression',\n",
            "       'anxiety_level', 'self_esteem', 'mental_health_history', 'depression',\n",
            "       'headache', 'blood_pressure', 'sleep_quality', 'breathing_problem',\n",
            "       'noise_level', 'living_conditions', 'safety', 'basic_needs',\n",
            "       'academic_performance', 'study_load', 'teacher_student_relationship',\n",
            "       'future_career_concerns', 'social_support', 'peer_pressure',\n",
            "       'extracurricular_activities', 'bullying', 'stress_level', 'Student ID',\n",
            "       'Academic Performance (GPA)', 'Study Hours Per Week',\n",
            "       'Social Media Usage (Hours per day)',\n",
            "       'Sleep Duration (Hours per night)',\n",
            "       'Physical Exercise (Hours per week)', 'Family Support  ',\n",
            "       'Peer Pressure', 'Relationship Stress', 'Mental Stress Level',\n",
            "       'Counseling Attendance', 'Diet Quality', 'Stress Coping Mechanisms',\n",
            "       'Cognitive Distortions', 'Family Mental Health History',\n",
            "       'Medical Condition', 'Substance Use', 'Timestamp', 'Country',\n",
            "       'Occupation', 'self_employed', 'family_history', 'treatment',\n",
            "       'Days_Indoors', 'Growing_Stress', 'Changes_Habits',\n",
            "       'Mental_Health_History', 'Mood_Swings', 'Coping_Struggles',\n",
            "       'Work_Interest', 'Social_Weakness', 'mental_health_interview',\n",
            "       'care_options'],\n",
            "      dtype='object')\n",
            "Error: No specified feature columns were found in the DataFrame.\n",
            "Please check the column names in your datasets and update the feature lists in preprocess_data.\n",
            "Model training skipped due to missing data or preprocessor.\n",
            "Model Performance Results:\n"
          ]
        }
      ]
    }
  ]
}